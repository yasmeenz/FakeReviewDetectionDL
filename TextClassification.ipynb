{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "198752bb-ad15-4c32-abf5-deeaebd25a66",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ed75d7c-d3ff-4d80-80c8-b71794fcc756",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yasme\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "352e8b18-57c1-4b9b-a76f-e1a1a43f3d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2a2718-4fbe-4254-9d8c-00e63510864a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup Pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a945d552-935b-40d2-944f-b821bcb40eca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\yasme\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0...\u001b[0m\n",
      "\u001b[1mDataset imdb_reviews downloaded and prepared to C:\\Users\\yasme\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset, info = tfds.load('imdb_reviews', with_info=True,as_supervised=True)\n",
    "\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e20f8fd-71b2-4ad3-946e-d3499f5d8422",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset, info = tfds.load('imdb_reviews', with_info=True,as_supervised=True)\n",
    "\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "768e5466-d3af-4f20-9628-0a2534da8ddc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
      "label:  0\n"
     ]
    }
   ],
   "source": [
    "for example, label in train_dataset.take(1):\n",
    "    print('text: ', example.numpy())\n",
    "    print('label: ', label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b31aa94f-2ae5-465b-a06f-cd7186cb7903",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53ec1943-71f3-408b-a413-0755c180d807",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ed6db0f-ee04-4c35-8aa4-c558562863bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts:  [b'I\\'ve rented and watched this movie for the 1st time on DVD without reading any reviews about it. So, after 15 minutes of watching I\\'ve noticed that something is wrong with this movie; it\\'s TERRIBLE! I mean, in the trailers it looked scary and serious!<br /><br />I think that Eli Roth (Mr. Director) thought that if all the characters in this film were stupid, the movie would be funny...(So stupid, it\\'s funny...? WRONG!) He should watch and learn from better horror-comedies such as:\"Fright Night\", \"The Lost Boys\" and \"The Return Of the Living Dead\"! Those are funny!<br /><br />\"Cabin Fever\" is THE reason why I registered to www.IMDb.com so I can release my thoughts of discontent about it.<br /><br />I\\'ve decided to watch the movie a second time >AAARGH!< and make notes for my partial \"review\" to show how foolish the movie is. \"Resident Evil\" (horror) or \"Dude, Where\\'s My Car?\" (comedy) I can watch over and over again and still enjoy! But this...!<br /><br />How bad can a script and a director be??? This bad. Here are the awful scenes in chronological order:<br /><br />In the early scenes we see Henry, who doesn\\'t realize his dog lying on the ground with its tongue hanging out of its mouth and dead-glazed stare is dead!<br /><br />The movie doesn\\'t explain anything about the blonde long-haired kid who like to bite people.<br /><br />And my answer to Marcy\\'s unanswered question (\"What\\'s wrong with the woods?\") is \"nothing\". The script has that bearded guy warn them about the woods just for \"suspense\".<br /><br />Then the \"smartest\" of the 5-pack, Bert, almost gives us an example of how to start a forest fire. He meets now-infected Henry who begs for help and from here on the movie wants to break the record in using the \"F-word\". Bert starts to freak out because Henry looks awfully ill. Bert:\"Don\\'t make me shoot you!\" (he forgets to add: \"...with my BB gun!\")<br /><br />Bert heads back to the cabin but how about that? He meets Marcy and Jeff who were having sex, but now suddenly decide to go out for a walk! Marcy wisely takes out the unguarded campfire Bert had started earlier (A moment of clarity for a change?) Bert doesn\\'t mention a word about Henry because the fool thinks he has killed him with his BB gun.<br /><br />Later, as the Five Estupidos sit around their campfire, another weirdo shows up with his dog. (Maybe that\\'s what the warning about the woods is all about? It\\'s filled with weirdos...and their dogs!?) They let him sit with them only because he has a huge bag filled with cannabis. (Their brains are completely intoxicated! No wonder why they are all so DUMB!) This is the last time we\\'ll see this forgettable character...alive!<br /><br />Henry shows up at their cabin, (NOTE: He was lying all the time a few yards from their cabin!!!) looking worse, almost like a zombie, covered in goo! He says he needs a doctor. But the Young Einsteins refuse to help the poor sucker. He gets into their unlocked truck which of course also has its key in the ignition. Henry almost seems smart enough to drive the Hell away from there but instead starts puking blood all over the dashboard, seats and windows. The Fantastic Five come out running, armed with: a BB Gun, a knife, a baseball bat (*huh? Ever tried playing baseball in the middle of the woods???), a poker, and a (insecticide?) spray-can, ready to combat the single, unarmed and terribly sick man. (clever script!) Bert manages to kill the car with one single shot of his BB Gun, which is only possible in the mind of director Eli Roth. \"What else am I supposed to do?\" Bert yells in his defense. Jeff and Paul try to knock Henry down with their bat and poker but miss and crash the truck\\'s windows instead. Henry walks up to the dumb girls who say: \"He\\'s coming towards us!\" (Thanks for the info, dumb broads, I can see that! But I don\\'t think he wants to do you any harm!) Marcy sprays in his eyes, making Henry yelp! And our \"hero\" Paul touches Henry\\'s arm with a burning log from the campfire, which they recklessly left burning while they were INSIDE the cabin! (Where has all that wisdom gone? I guess the cannabis had started to take its toll!:-) Henry turns into The Human Torch and runs away, screaming.<br /><br />The following day, Bert and Jeff head out for a mechanic. And Marcy decides to \"go for help\" all by herself, in the woods, as Paul stays behind with Karen...Doesn\\'t that sound idiotic? Marcy could have stayed with Karen and Paul because Jeff and Bert were already \"going for help\"!<br /><br />I skip my comments now to how we suddenly see Marcy in a CANOE rowing over a huge and winding river! How did she get a canoe? Does she even know where she\\'s going!? Anyway, she goes to the riverbank and finds a very big and seemingly abandoned cabin and, like in most horror movies, walks inside the cabin saying:\"Hello? Is anyone there?\" Bert suddenly pops up from behind a furniture and scares her (and me at first). And along comes Jeff, as well. How did THEY get here!? Did they swim across the river??? Do you see how brainless the script is!?<br /><br />Deputy Winston meets Paul at the cabin. He somehow doesn\\'t notice the blood on their truck. This happens around 35 minutes of viewing and I have decided to stop torturing myself anymore and popped the DVD out. (Before I take my own eyes out!...Now, THAT\\'s funny!)<br /><br />If you liked this movie, do yourself a favor and watch \"Fright Night\", \"The Lost Boys\", or \"The Return of The Living Dead\". Then you\\'ll see they are MORE entertaining than this...thing. Even the \"Toxic Avenger part 2\", which is also a lousy film, is way MORE funnier than \"Cabin Fever\".'\n",
      " b'This musical was not quite what I expected, foremost being there weren\\'t many scenes between Brando and Sinatra. As it was based on a Damon Runyon story, I expected irony and surprise, of which there was one really good one - when we find that Sinatra\\'s gang has used the Salvation Army office for their crap game while Brando was in Havana with Simmons. If course it comes at the right moment too, when Brando brings her back. I really didn\\'t expect much from Brando as a singer, but he surprised me. He wasn\\'t great but he was just fine in the role. His big number in the sewer, however, with the rest of Sinatra\\'s boys was the only place I felt Brando\\'s voice was weak. He just didn\\'t have the power the grand climax demanded. Overall I found the scenes between Brando and Simmons to be filled with electricity, something I didn\\'t think would happen when we first see Simmons by herself, and later when we\\'re introduced to Brando in the restaurant with Sinatra trying to pull a fast one on him. It wasn\\'t until Brando goes to her office that the story came to life. <br /><br />Frank Sinatra, on the other hand, was flat, even his vocal performances. And Vivian Blaine, who I never heard of, but who I guess played the role on Broadway, just seemed to slow the proceedings down. The scenes between her and Sinatra were obvious. Also, her songs felt the weakest to me both in terms of advancing story or character. On top of that, all the Goldwyn Girls numbers seemed shoe horned in, just there for glitz. For example, when Frank meets with Brando in the nightclub, we just cut to the stage routine for the cat number - then it cuts back to the guys who continue on as if there hadn\\'t been any dance number at all. Whenever Brando and Simmons were on screen, I was having a great time, but each time we return to the Sinatra-Blaine story, my interest level waned. <br /><br />As for the songs, there were some good ones, particularly the very first number with Stubby Kaye, the Fugue for Tinhorns number (Can Do!). That\\'s a great song and it reminded me of the very first song in The Music Man - Cash for the Merchandise... whatever it\\'s called. And the number in the sewer - I couldn\\'t help but be reminded of \"Cool\" from West Side Story - which brings me to a point. I really did not like the art direction in this film. The fake Times Square was just so completely phony it drew attention to itself. Same for the Havana sequence, and particularly the sewer. I realize back in 1955 most musicals were shot on sets, but things were changing - Carousel, for example, made great use of location photography. Even On the Town shot scenes in Mahattan in 1949. By the time we get to West Side Story in 1961 it\\'s a given that stuff taking place in Manhattan had to be actually shot in Manhattan. So by comparison, Guys and Dolls set-bound Manhattan felt dated and more than a little too cute. And changing Lindy\\'s to Mindy\\'s - did they really have to do that for legal reasons? Now, I always thought Guys and Dolls was a musical about Sinatra and Brando and their adventures with various girls. It was much more focused than that, which is to its credit. In that regard it is much better than Les Girls, which was interesting in it\\'s own right, but had a certain shallowness to it. <br /><br />My one major complaint about Guys and Dolls, and I don\\'t know if this is endemic to the original stage show, but when Jean Simmons realizes that Brando never took any money for a bet that he made with Sinatra and even said that he lost the bet, she just runs off to find him and we cut to the wedding. It seems to me a scene between Brando and Simmons would have added to the impact of the story. To see Brando come around as she came around to him would have been a great scene. There is such a scene in The Music Man (SPOILERS AHEAD), when Harold and Marion have that duet while he\\'s waiting for her to change. She\\'s upstairs in her house, he\\'s down on the sidewalk. He\\'s singing 76 Trombones. She\\'s singing Goodnight My Someone. They suddenly switch and sing each other\\'s songs - a beautiful way to convey their cross over to each other. It\\'s an emotional high moment of the film. Still, Guys and Dolls had a lot going for it.'\n",
      " b\"I really enjoyed this one, and although the ending made me angry, I still give it 10 out of 10.<br /><br />Four college girls (Baltron, Kelly, Stahl and Cadby) are driving down to Florida, on their way they meet 2 guys (Turner, Davis), they really add nothing to the plot, but are at least somewhat likable. The girls agree to meet the guys in Florida for some fun, but they have car problems and never make it. One of the girls decides to go to a nearby gas station for help, the other three stay by the car.<br /><br />Soon one of the girls has to use the bathroom, being in the middle of nowhere she has no choice but to go in the bushes. Soon she witnesses as a man (March) strangles a woman, in terror the girl flees the area, she doesn't get very far, but manages to get lost.<br /><br />Her friends by the car go looking for her, they too go into the woods and run into the same man, one of them sees the dead woman, the man responds by shooting the girls head off, the other girl runs away, manages to make it back to the car where she is also killed.<br /><br />Eventually the two remaining girls find each other and because they break into the gas station get arrested. This is when I started getting mad, these poor girls are afraid for their lives and the redneck cops don't believe them.<br /><br />They are treated badly and one of them is left alone for the madman to kill her in the cell, the remaining friend manages to escape, but not without getting in dangerous situations.<br /><br />This movie has nudity, good actresses, a shower scene imitating Psycho, graphic violence towards women and solid story. Some women will probably find it offensive and sensitive individuals will NOT like the ending, but over all, this is a great little unknown movie.\"]\n",
      "\n",
      "labels:  [0 1 1]\n"
     ]
    }
   ],
   "source": [
    "for example, label in train_dataset.take(1):\n",
    "  print('texts: ', example.numpy()[:3])\n",
    "  print()\n",
    "  print('labels: ', label.numpy()[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8cb06d-d8d4-4bfc-9f97-3c8379e2d3aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# The Encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88790aaa-1743-44d9-bcaf-ca07dec19a59",
   "metadata": {
    "tags": []
   },
   "source": [
    "The encoder is the first layer in a model that is used to convert text into a sequence of token indices.\n",
    "In natural language processing (NLP), text is typically represented as a sequence of tokens, where each token represents a word or a subword unit.\n",
    "Token indices are numerical representations of these tokens, which are used by the model to process and understand the text.\n",
    "The process of converting text into token indices is known as tokenization.\n",
    "Tokenization is an important step in NLP tasks as it allows the model to work with discrete units of text instead of treating the entire text as a continuous sequence.\n",
    "The encoder takes the input text and applies tokenization to convert it into a sequence of token indices.\n",
    "Each token index represents a specific token in the text and is typically a unique integer value.\n",
    "These token indices are then used as input to the subsequent layers of the model for further processing and analysis.\n",
    "The encoder can use various tokenization techniques, such as word-based tokenization or subword-based tokenization, depending on the specific requirements of the task.\n",
    "The choice of tokenization technique can impact the performance of the model, as it determines how the text is divided into tokens and represented as token indices.\n",
    "Overall, the encoder plays a crucial role in NLP models by converting text into a format that can be effectively processed and analyzed by the subsequent layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a161111-27c3-42f8-a7c5-03abd46dfd19",
   "metadata": {},
   "source": [
    "**After the encoder is an embedding layer**: \n",
    "- An embedding layer is a component in a neural network that is used to convert discrete word indices into continuous vector representations.\n",
    "- In natural language processing tasks, words are typically represented as one-hot vectors, where each word is represented by a vector with all zeros except for a single one at the index corresponding to the word's position in the vocabulary.\n",
    "- However, one-hot vectors are not suitable for input into neural networks because they are high-dimensional and sparse, meaning that most of the elements in the vector are zeros.\n",
    "- An embedding layer solves this problem by mapping each word index to a dense vector of continuous values, also known as word embeddings.\n",
    "- These word embeddings capture semantic relationships between words, allowing the neural network to better understand the meaning of the words in the input sequence.\n",
    "\n",
    "**An embedding layer stores one vector per word**:\n",
    "- In the embedding layer, each word in the vocabulary is associated with a unique vector representation.\n",
    "- These vectors are typically of fixed length and have continuous values.\n",
    "- The length of the vectors is a hyperparameter that needs to be specified before training the neural network.\n",
    "- The dimensionality of the word embeddings determines the amount of information that can be captured about each word.\n",
    "\n",
    "**When called, it converts the sequences of word indices to sequences of vectors**:\n",
    "- The embedding layer takes as input a sequence of word indices, where each index represents a word in the input sequence.\n",
    "- It then looks up the corresponding word embeddings for each word index and returns a sequence of vectors.\n",
    "- This conversion from word indices to vectors is done by performing a table lookup operation, where the word index is used as an index to retrieve the corresponding word embedding from a lookup table.\n",
    "\n",
    "**These vectors are trainable**:\n",
    "- The word embeddings in the embedding layer are trainable parameters of the neural network.\n",
    "- During the training process, the neural network adjusts the values of the word embeddings based on the objective function and the gradients computed during backpropagation.\n",
    "- This allows the neural network to learn meaningful representations for the words in the input sequence that are useful for the task at hand.\n",
    "\n",
    "**After training (on enough data), words with similar meanings often have similar vectors**:\n",
    "- One of the key advantages of using an embedding layer is that it allows words with similar meanings to have similar vector representations.\n",
    "- This property is learned during the training process, where the neural network is exposed to a large amount of data and learns to associate words that appear in similar contexts with similar vector representations.\n",
    "- As a result, words that are semantically related or have similar meanings tend to have similar vector representations in the embedding space.\n",
    "- This can be useful for various natural language processing tasks, such as word similarity measurement, word analogy completion, and sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce2f14d-b36a-46e7-8d3a-1091b968fe5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e28a7ef9-91e3-4bf5-bcad-747ec79b5474",
   "metadata": {},
   "source": [
    "**Recurrent Neural Network (RNN)**\n",
    "\n",
    "- A recurrent neural network (RNN) is a type of artificial neural network that is designed to process sequential data.\n",
    "- Sequential data refers to data that has a specific order or sequence, such as time series data or sentences in natural language processing.\n",
    "- RNNs are particularly useful for tasks that involve processing and understanding sequential patterns, such as speech recognition, language translation, and sentiment analysis.\n",
    "\n",
    "**Processing Sequence Input**\n",
    "\n",
    "- When an RNN processes sequence input, it does so by iterating through the elements of the sequence one by one.\n",
    "- Each element of the sequence is considered at a specific timestep, which represents a specific point in the sequence.\n",
    "- The RNN starts by processing the first element of the sequence at the first timestep, then moves on to the second element at the second timestep, and so on.\n",
    "\n",
    "**Passing Outputs to Inputs**\n",
    "\n",
    "- One key characteristic of RNNs is that they pass the outputs from one timestep to their input on the next timestep.\n",
    "- This means that the output of the RNN at a particular timestep becomes part of the input for the next timestep.\n",
    "- By doing this, the RNN is able to capture and remember information from previous timesteps, allowing it to learn and understand the sequential patterns in the data.\n",
    "\n",
    "**Example**\n",
    "\n",
    "- Let's consider an example of using an RNN for language translation.\n",
    "- Suppose we have a sentence in English, \"I love cats,\" and we want to translate it into French.\n",
    "- The RNN would process the sentence one word at a time, starting with the word \"I\" at the first timestep.\n",
    "- The output of the RNN at the first timestep would then become part of the input for the second timestep, where the word \"love\" is processed.\n",
    "- This process continues until the entire sentence has been processed, and the RNN outputs the translated sentence in French.\n",
    "\n",
    "**Summary**\n",
    "\n",
    "- In summary, an RNN is a type of neural network that is designed to process sequential data.\n",
    "- It does so by iterating through the elements of the sequence one by one at different timesteps.\n",
    "- The outputs of the RNN at each timestep are passed as inputs to the next timestep, allowing the network to capture and remember information from previous timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ad956c-d324-4183-9f54-58b091f28ec9",
   "metadata": {},
   "source": [
    "## Create the text encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fafd122-a7f1-484c-8d45-e6abe4ea3bc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yasme\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yasme\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yasme\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yasme\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(train_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ff1281c-b00b-4a5d-9410-830717c121c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i',\n",
       "       'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but'],\n",
       "      dtype='<U14')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.array(encoder.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f66304c-d23d-4efb-baa2-5f1a1670ec5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d4cd47-7e2c-4408-a9e1-03d630e42be1",
   "metadata": {},
   "source": [
    "## Create the Model\n",
    "<img src=\"https://www.tensorflow.org/static/text/tutorials/images/bidirectional.png\" alt=\"A drawing of the information flow in the model\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "917d7117-c372-4112-8ed5-68ca9e868f71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4edea63a-374c-4fdc-8a29-c0e4d8f23c3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "print([layer.supports_masking for layer in model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92bca990-aff9-43e2-a581-2c574ce75de8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 6s 6s/step\n",
      "[-0.00681133]\n"
     ]
    }
   ],
   "source": [
    "# predict on a sample text without padding.\n",
    "\n",
    "sample_text = ('The movie was cool. The animation and the graphics '\n",
    "               'were out of this world. I would recommend this movie.')\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f39b97f-bfba-4014-92d1-3cc3cd9bd4cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 187ms/step\n",
      "[-0.00681133]\n"
     ]
    }
   ],
   "source": [
    "# predict on a sample text with padding\n",
    "\n",
    "padding = \"the \" * 2000\n",
    "predictions = model.predict(np.array([sample_text, padding]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92ed155-4af9-4418-8452-c564315397c3",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71c69ac-b83e-49bc-98d6-33eb26d26e34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\yasme\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yasme\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 374s 927ms/step - loss: 0.6431 - accuracy: 0.5618 - val_loss: 0.5059 - val_accuracy: 0.7651\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 354s 898ms/step - loss: 0.4045 - accuracy: 0.8152 - val_loss: 0.3881 - val_accuracy: 0.8406\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 362s 925ms/step - loss: 0.3434 - accuracy: 0.8523 - val_loss: 0.3466 - val_accuracy: 0.8417\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 415s 1s/step - loss: 0.3240 - accuracy: 0.8611 - val_loss: 0.3396 - val_accuracy: 0.8599\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 413s 1s/step - loss: 0.3133 - accuracy: 0.8674 - val_loss: 0.3269 - val_accuracy: 0.8568\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 37871s 97s/step - loss: 0.3079 - accuracy: 0.8705 - val_loss: 0.3286 - val_accuracy: 0.8625\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 47228s 121s/step - loss: 0.3015 - accuracy: 0.8723 - val_loss: 0.3231 - val_accuracy: 0.8578\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 375s 960ms/step - loss: 0.2994 - accuracy: 0.8739 - val_loss: 0.3321 - val_accuracy: 0.8443\n",
      "Epoch 9/10\n",
      "236/391 [=================>............] - ETA: 2:40 - loss: 0.2964 - accuracy: 0.8737"
     ]
    }
   ],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_dataset, epochs=10,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb23426-138a-4450-aa58-1e46c04020c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_graphs(history, 'loss')\n",
    "plt.ylim(0, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85d844d-5c50-4821-92bd-124971267a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = ('The movie was cool. The animation and the graphics '\n",
    "               'were out of this world. I would recommend this movie.')\n",
    "predictions = model.predict(np.array([sample_text]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410a3ba4-c260-4798-bd00-bf203c830f97",
   "metadata": {},
   "source": [
    "## Stack two or more LSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97719d88-0671-4154-9ddb-484f7af8d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_dataset, epochs=10,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc05b07-fa20-49e3-b818-51d15b54a52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2bfe9f-c578-4878-b58c-4b67382e584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_graphs(history, 'loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
